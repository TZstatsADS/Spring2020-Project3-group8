---
title: "Can you recognize the emotion from an image of a face?"
author: "Jiancong Shen, Vikki Sui, Jinxu Xiang, Ruiqi Xie, Wenjie Xie"
output:
  html_document:
    df_print: paged
---

## Abstract

This document mainly describes an improved way of emotion recognition algorithm. This method reduces the calculation time and improves the classification accuracy by pre-transforming data, extracting efficient features (base on points and images), and improving the algorithm. The goal is to select the best model by comparing the classification results of different classifiers under different features.

###  Computer configuration

System: Microsoft Windows 10 x64

CPU: Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz(3600 MHz)

GPU: NVIDIA GeForce RTX 2070 SUPER (8192 MB)

RAM: 16.00 GB (2400 MHz)

### Final result

|       | test accuracy | model training time | all training time | test prediction time | all prediction time | 
| ------ | ------ | ------ | ------ | ------ | ------ | 
| gbm_baseline | 44.7% | 1960s | 1961s | 13.3s | 13.4s |
| gbm_improved | 47.7% | 42.9s | 67.2s | 0.03s | 5.92s | 
| gbm_colored | 48.8% | 45.2s | 188s | 0.04s | 38.6s |
| svm_improved | 57.1% | 2.39s | 26.7s | 0.20s | 6.09s |
| svm_colored | 60.5% | 2.38s | 146s | 0.20s | 38.8s |
| xgb_improved | 52.9% | 45.2s | 69.5s | 0.06s | 5.95s |
| xgb_colored | 56.3% | 31.7s | 175s | 0.06s | 38.6s |

Based on the fact that the total training time is the sum of model training time, the feature extraction time, and the data processing time, we have the formula below:

All training time = Training data preprocessing time + Feature extraction time + Model training time

All test time = Testing data preprocessing time + Feature extraction time + Test prediction time

The accuracy and time shown in the table above are from a certain measurement and may not exactly match the results shown in the PDF document.

The improved feature contains 129 features extracted from 78 points' position. The color feature contains all improved feature and 10 other features which are extracted from points and images. Therefore, the total dimension of color feature is 139.

We extract color feature and use the svm classifier as our improved model. Since there is no other results affect our judgment of in-class tests. We predict the final accuracy of in-class test to be 58%.

### Difference between Rmd and Html

Since the baseline model is large, we can't upload it to github, so we set the run.baseline to false after knit. If you need to run Rmd, the result will be slightly different, it will not contain baseline model, but the rest part is same as Html file. If you want to see the result of baseline, you have to set run.baseline and run.train.gbm to true, it will take more than half an hour to calculate. Moreover, before running Rmd, you should change setwd() below to your path.

```{r library, message=FALSE}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}
if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}
if(!require("ggplot2")){
  install.packages("ggplot2")
}
if(!require("caret")){
  install.packages("caret")
}
if(!require("gbm")){
  install.packages("gbm")
}
if(!require("e1071")){
  install.packages("e1071")
}
if(!require("geometry")){
  install.packages("geometry")
}
if(!require("mlogit")){
  install.packages("mlogit")
}
if(!require("tidyverse")){
  install.packages("tidyverse")
}
if(!require("xgboost")){
  install.packages("xgboost")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(gbm)
library(e1071)
library(geometry)
library(mlogit)
library(tidyverse)
library(xgboost)  
```

## Step 0 set work directories

```{r set wd and seed, eval=FALSE}
set.seed(1)
setwd("C:/course/5243 Applied Data Science/Project/Spring2020-Project3-group8/doc")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

Provide directories for training images. Training images and training fiducial points will be in different subfolders. 

```{r data path}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```


## Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) process for baseline
+ (T/F) process for data set
+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training and test set
+ (T/F) process model of training data

```{r boolean value}
run.baseline = FALSE # run baseline feature, training slow!

run.dataprocess.fiducial_pt_list = FALSE # save fiducial_pt_list
run.dataprocess.all_points = FALSE # save all_points, new data after rotate zoom and move
run.dataprocess.ave_points = FALSE # save ave_points, average data of 22 emotions
run.dataprocess.diff_points = FALSE # save diff, distance of each point to its mean

run.cv = FALSE # run cross-validation on the training set
K = 5  # number of CV folds
run.feature.train = FALSE # process features for training set
run.feature.test = FALSE # process features for test set

run.train.gbm = FALSE # run gbm training model
run.train.svm = FALSE # run svm training model
run.train.xgb = FALSE # run xgb training model
```

The cross-validation method takes too much time, and the final model we choose does not need it. Although it works well, we will not run cross-validation process in this document.

## Step 2: import data and train-test split 

This time we did not use a random 20% photos as the test set, but instead randomized all the pictures of 20% of the people as the test set. Because when we deal with the problem of facial expression recognition, there is a high probability of encountering strangers. So the expectation of this selection method is closer to the real classification accuracy.

However, we only have 2500 pictures and 230 people, which is not enough for accurate training. When we randomly select 20% of the people, it is likely that there is too little training data for a certain expression in the training set. This may increase the variance of classification accuracy.

This effect is limited to the tests in this document. The tests we need to do will use all 2,500 training sets, the expectations and variances mentioned above have nothing to do with it.

```{r train and test data}
#train-test split
info <- read.csv(train_label_path)
n <- length(info$identity %>% unique())

#take 80% of the observations as the train set
n_train <- round(n*(4/5), 0)
train_identity = sample(info$identity %>% unique(), n_train, replace = F)
train_idx <- which(info$identity %in% train_identity)
test_idx <- setdiff(info$Index,train_idx)
train_idx <- sample(train_idx, length(train_idx), replace = F)
```

Fiducial points are stored in matlab format. In this step, we read them and store them in a list.

```{r dp fpl}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
if(run.dataprocess.fiducial_pt_list){
  
  n_files <- length(list.files(train_image_dir))
  readMat.matrix <- function(index){
     return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
  }
  
  #load fiducial points
  fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
  save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
}
```

```{r dp allp}
#function to rotate, translate, zoom the fiducial points to standardize all the points
#take the middle height of two eyes and the height of nose center to standardize this distance to be 170

#input: indices, train_dir, fiducial_pt_list, single, double
#output: the fiducial points after all the trasformation
if(run.dataprocess.all_points){
  
  source("../lib/change_images.R")
  load("../output/fiducial_pt_list.RData")
  
  single = c(35,36,37,38,44,52,56,59,62)  #points without a match, i.e, points on the axis of symmetry
  double = data.frame(
    x1 = c(1,2,3,4,5,6,7,8,9,19,20,21,22,23,24,25,26,39,40,41,42,43,50,51,57,58,63),
    x2 = c(10,15,14,13,12,11,18,17,16,31,30,29,28,27,34,33,32,49,48,47,46,45,54,53,55,60,61))
  #points that matches each other on both sides of the face
  
  indices = 1:2500
  
  tm.run.all_points = system.time(all_points <- change_points(indices, train_dir, fiducial_pt_list, single, double))
  
  all_points = map(all_points, as.matrix)
  
  save(all_points, file = "../output/all_points.RData")
  save(tm.run.all_points, file = "../output/tm.run.all_points.RData")
}
```

```{r dp avep}
#data manipulation to find the average points of each emotion, in order to find more detailed differences of each emotion
#input: all_points
#output: ave_points
if(run.dataprocess.ave_points){
  
  load("../output/all_points.RData")
  
  emo_idx = map(1:22, ~info$emotion_idx == .x)
  group_points = map(emo_idx, ~all_points[.x])
  ave_points = NULL
  for (i in 1:length(group_points)){
    mean = matrix(rep(0,78*2), nc = 2)
    l = length(group_points[[i]])
    for(j in 1:l){
      group_points[[i]][[j]]
      mean = mean + group_points[[i]][[j]]/l
    }
    ave_points = c(ave_points, list(mean))
  }
  save(ave_points, file="../output/ave_points.Rdata")
}
```

```{r dp dist and dp}
if(run.dataprocess.diff_points){
  
  load("../output/all_points.RData")
  load("../output/ave_points.RData")
  
  #functions to find the difference on x-aixs and y-axis of each point to corresponding points of the 22 average points of different emotions
  #input: emo_index, all_points
  #output: diff
  get_diff <- function(emo_index, train= all_points){
    group <- info %>% filter(emotion_idx == emo_index)
    idx <- group$Index
    points <- train[idx]
    dfmean <- ave_points[[emo_index]]
    diff <- map(points, function(x){x-dfmean})
    return (diff)
  }
  diff_points <- list()
  
  for (i in 1:22){
    diff_points <- c(diff_points, get_diff(i, all_points))
  }
  
  #find the the distance of each point to the corresponding points of the 22 average points of different emotions
  #output: distance
  distance <- map(diff_points, function(x){x[,1]^2+x[,2]^2})
  distance = map(1:78, function(y) map(distance, ~.x[y]) %>% unlist)
  
  save(diff_points, file="../output/diff_points.Rdata")
  save(distance, file="../output/distance.Rdata")
}
load("../output/diff_points.Rdata")
load("../output/distance.Rdata")
```


## Step 3: construct features and responses

+ feature_baseline: function that represent the original features given by the distance of 78 points between each other 

+ feature: function that takes the updated feature that were selected by ourselves, which has 129 dimensions if we exclude the color feature and 139 dimension if we include the color feature
  + feature function has a boolean input which indicate if we want to take color features or not
  + color feature includes the wrinkle between eyes and the nasolabial folds
  + with the color feature, the running time will be longer but the predicting accuracy will increase
  + without the color feature, the running time will be shorter but the predicting acuuracy will decrease

+ we will also the features into the output folder

```{r feature extraction}
source("../lib/feature_baseline.R")
source("../lib/feature.R")
load("../output/all_points.RData")

#get the time required for taking the features of train set and test set using the original feature function which is simply taking the distance of each two points
if(run.baseline){
  tm_feature_train_baseline <- NA
  if(run.feature.train){
    tm_feature_train_baseline <- system.time(dat_train_baseline <- feature_baseline(fiducial_pt_list, train_idx))
    save(dat_train_baseline, file="../output/feature_train_baseline.RData")
    save(tm_feature_train_baseline, file="../output/tm_feature_train_baseline.RData")
  }
  
  tm_feature_test_baseline <- NA
  if(run.feature.test){
    tm_feature_test_baseline <- system.time(dat_test_baseline <- feature_baseline(fiducial_pt_list, test_idx))
    save(dat_test_baseline, file="../output/feature_test_baseline.RData")
    save(tm_feature_test_baseline, file="../output/tm_feature_test_baseline.RData")
  }
}

#time required for taking the features of train set using the updated feature function with and without the color feature
tm_feature_train <- NA
tm_feature_train_color <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx, image_file = "../data/train_set/images/", all_points, colorfeature = FALSE))
  save(dat_train, file="../output/feature_train.RData")
  save(tm_feature_train, file="../output/tm_feature_train.RData")
  
  tm_feature_train_color <- system.time(dat_train_color <- feature(fiducial_pt_list, train_idx, image_file = "../data/train_set/images/", all_points, colorfeature = TRUE))
  save(dat_train_color, file="../output/feature_train_color.RData")
  save(tm_feature_train_color, file="../output/tm_feature_train_color.RData")
}

#time required for taking the features of test set using the updated feature function with and without the color feature
tm_feature_test <- NA
tm_feature_test_color <- NA
if(run.feature.test){
  tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx, image_file = "../data/train_set/images/", all_points, colorfeature = FALSE))
  save(dat_test, file="../output/feature_test.RData")
  save(tm_feature_test, file="../output/tm_feature_test.RData")
  
  tm_feature_test_color <- system.time(dat_test_color <- feature(fiducial_pt_list, test_idx, image_file = "../data/train_set/images/", all_points, colorfeature = TRUE))
  save(dat_test_color, file="../output/feature_test_color.RData")
  save(tm_feature_test_color, file="../output/tm_feature_test_color.RData")
}

```

## Step 4: Train a classification model with training features and responses

In this step, we will train the model using GBM, SVM, and XGB, respectively using baseline features, improved features and improved features with color features to get the training model, then save these model to the output folder.

### 1.Baseline model with dist feature in GBM

```{r train gbm baseline}
#use the baseline features to train the gbm model, get the model called gbm_model_baseline and the time used called tm.gbm.train_baseline
#save the output model into the output
if(run.baseline){
  load("../output/feature_train_baseline.RData")
  if(run.train.gbm){
    source("../lib/train_gbm.R")
    gbm_result_train_baseline = gbm_train(dat_train_baseline, n.trees = 200, bag.fraction = 0.8, shrinkage = 0.1, cv.folds = 3)
    gbm_model_baseline = gbm_result_train_baseline[[1]]
    tm.gbm.train_baseline = gbm_result_train_baseline[[2]]
    save(gbm_model_baseline, file="../output/gbm_model_baseline.RData")
    save(tm.gbm.train_baseline, file="../output/tm.gbm.train_baseline.RData")
  }
}
```

### 2.Improved feature in GBM

```{r train gbm improved}
#use the improved features to train the gbm model, get the model called gbm_model and the time used called tm.gbm.train
#save the output model into the output
if(run.train.gbm){
  source("../lib/train_gbm.R")
  load("../output/feature_train.RData")
  
  gbm_result_train = gbm_train(dat_train, n.trees = 200, bag.fraction = 0.8, shrinkage = 0.1, cv.folds = 3)
  gbm_model = gbm_result_train[[1]]
  tm.gbm.train = gbm_result_train[[2]]
  save(gbm_model, file="../output/gbm_model.RData")
  save(tm.gbm.train, file="../output/tm.gbm.train.RData")
}
```

### 3.Improved feature in SVM

```{r train svm improved}
#use the improved features to train the svm model, get the model called svm_model and the timeused called tm.svm.train
#save the output model into the output
if(run.train.svm){
  source("../lib/train_SVM.R")
  load("../output/feature_train.RData")
  
  svm_result_train = svm_train(dat_train, kernel = 'poly', degree = 1, gamma = 0.008)
  svm_model = svm_result_train[[1]]
  tm.svm.train = svm_result_train[[2]]
  save(svm_model, file="../output/svm_model.RData")
  save(tm.svm.train, file="../output/tm.svm.train.RData")
}
```

### 4.Improved feature in XGB

```{r train xgb improved cv1}
#use cross validation to find the best parameter for the xgb model
if(run.cv & run.train.xgb){
  source("../lib/xgb_tune.R")
  load("../output/feature_train.RData")
  
  depth = c(5,10,15)
  child = c(3,5,10)
  xgb_result_cv = xgb_tune(dat_train, depth, child, K)
  xgb_err = xgb_result_cv[[1]]
  tm.xgb.cv = xgb_result_cv[[2]]
  xgb_err_tune = xgb_err[[1]] %>% as.data.frame()
  xgb_best_par = xgb_err[[2]] %>% as.data.frame()
}
```

```{r train xgb improved cv2}
if(run.cv & run.train.xgb){
  colnames(xgb_err_tune) = c(3,5,10)
  xgb_err_tune = gather(xgb_err_tune, key = "min_child")
  xgb_err_tune$depth = rep(c(5,10,15),3)
  xgb_err_tune
  ggplot(xgb_err_tune, mapping = aes(x=min_child, y=depth, fill=value))+
    geom_tile()
}
```

```{r train xgb improved}
#fit the xgb model with the best parameter if we use cross validation and with default if we do not use cross validation, and save the xgb_model into the output folder
if(run.train.xgb){
  source("../lib/xgb_train.R")
  load("../output/feature_train.RData")
  
  if(run.cv)
    xgb_result = xgb_train(dat_train, par = best_par_xgb)
  else
    xgb_result = xgb_train(dat_train)
  xgb_model = xgb_result[[1]]
  tm.xgb.train = xgb_result[[2]]
  save(xgb_model, file="../output/xgb_model.RData")
  save(tm.xgb.train, file="../output/tm.xgb.train.RData")
}

```

### 5.Improved feature with color in GBM

```{r train gbm colored}
#use the features with colors to train the gbm model called gbm_model_color, and save the model into output folder
if(run.train.gbm){
  source("../lib/train_gbm.R")
  load("../output/feature_train_color.RData")
  
  gbm_result_train_color = gbm_train(dat_train_color, n.trees = 200, bag.fraction = 0.8, shrinkage = 0.1, cv.folds = 3)
  gbm_model_color = gbm_result_train_color[[1]]
  tm.gbm.train_color = gbm_result_train_color[[2]]
  save(gbm_model_color, file="../output/gbm_model_color.RData")
  save(tm.gbm.train_color, file="../output/tm.gbm.train_color.RData")
}
```

### 6.Improved feature with color in SVM

```{r train svm colored}
#use the features with colors to train the svm model called svm_model_color and save the model into output folder
if(run.train.svm){
  source("../lib/train_SVM.R")
  load("../output/feature_train_color.RData")
  
  svm_result_train_color = svm_train(dat_train_color, kernel = 'poly', degree = 1, gamma = 0.008)
  svm_model_color = svm_result_train_color[[1]]
  tm.svm.train_color = svm_result_train_color[[2]]
  save(svm_model_color, file="../output/svm_model_color.RData")
  save(tm.svm.train_color, file="../output/tm.svm.train_color.RData")
}
```

### 7.Improved feature with color in XGB

```{r train xgb colored cv1}
#We can choose to use cross validation or not. If yes, we will need to run the two chunks below and get the best parameters, if not, we can skip the cross validation steps
if(run.cv & run.train.xgb){
  source("../lib/xgb_tune.R")
  load("../output/feature_train_color.RData")
  
  depth = c(5,10,15)
  child = c(3,5,10)
  xgb_result_cv_color = xgb_tune(dat_train_color, depth, child, K)
  xgb_err_color = xgb_result_cv_color[[1]]
  tm.xgb.cv_color = xgb_result_cv_color[[2]]
  xgb_err_tune_color = xgb_err_color[[1]] %>% as.data.frame()
  xgb_best_par_color = xgb_err_color[[2]] %>% as.data.frame()
}
```

```{r train xgb colored cv2}
if(run.cv & run.train.xgb){
  colnames(xgb_err_tune_color) = c(3,5,10)
  xgb_err_tune_color = gather(xgb_err_tune_color, key = "min_child")
  xgb_err_tune_color$depth = rep(c(5,10,15),3)
  xgb_err_tune_color
  ggplot(xgb_err_tune_color, mapping = aes(x=min_child, y=depth, fill=value))+
    geom_tile()
}
```

```{r train xgb colored}
#use the features with colors to train the xgb model called xgb_model_color and save the model into the output folder
if(run.train.xgb){
  source("../lib/xgb_train.R")
  load("../output/feature_train_color.RData")
  
  if(run.cv)
    xgb_result_color = xgb_train(dat_train_color, par = best_par_xgb)
  else
    xgb_result_color = xgb_train(dat_train_color)
  xgb_model_color = xgb_result_color[[1]]
  tm.xgb.train_color = xgb_result_color[[2]]
  save(xgb_model_color, file="../output/xgb_model_color.RData")
  save(tm.xgb.train_color, file="../output/tm.xgb.train_color.RData")
}

```


## Step 5: Run test on test images

For each model we constructed, we use that model to make prediction on the test set, then we give the confusion matrix of the prediction and the predicting accuracy repectively. 

### 1.Baseline model with dist feature in GBM

Due to the size limit of github. We can't upload 'gbm_model_baseline'. So we put the run.baseline to FALSE. If you want to run baseline. Please run model training of gbm in baseline first.

```{r test gbm baseline}
#prediction of the gbm model with only baseline features
if(run.baseline){
  source("../lib/test_gbm.R")
  load("../output/gbm_model_baseline.RData")
  load("../output/feature_test_baseline.RData")
  
  gbm_result_test_baseline = gbm_test(gbm_model_baseline, dat_test_baseline)
  gbm_pred_baseline = gbm_result_test_baseline[[1]]
  tm.gbm.test_baseline = gbm_result_test_baseline[[2]]
  gbm_pred_class_baseline = apply(gbm_pred_baseline, 1, which.max)
}
```

```{r cm gbm baseline}
#get the confusion matrix of the prediction
if(run.baseline){
  confusionMatrix(factor(gbm_pred_class_baseline), dat_test_baseline$emotion_idx)
}
```

```{r pa gbm baseline}
#accuracy of the above gbm model
if(run.baseline){
  gbm_accuracy_test_baseline = mean(dat_test_baseline$emotion_idx == gbm_pred_class_baseline)
  print(paste0("The accuracy for gbm model baseline is: ", gbm_accuracy_test_baseline))
}
```

### 2.Improved feature in GBM

```{r test gbm improved}
#prediction of the gbm model with improved features
source("../lib/test_gbm.R")
load("../output/gbm_model.RData")
load("../output/feature_test.RData")

gbm_result_test = gbm_test(gbm_model, dat_test)
gbm_pred = gbm_result_test[[1]]
tm.gbm.test = gbm_result_test[[2]]
gbm_pred_class = apply(gbm_pred, 1, which.max)
```

```{r cm gbm improved}
#get the confusion matrix of the prediction
confusionMatrix(factor(gbm_pred_class), dat_test$emotion_idx)
```

```{r pa gbm improved}
#get the accuracy of the prediction of gmb model with improved features
gbm_accuracy_test = mean(dat_test$emotion_idx == gbm_pred_class)
print(paste0("The accuracy for gbm model is: ", gbm_accuracy_test))
```

### 3.Improved feature in SVM

```{r test svm improved}
#prediction of the svm model with improved features
source("../lib/test_SVM.R")
load("../output/svm_model.RData")
load("../output/feature_test.RData")

svm_result_test = svm_test(svm_model, dat_test)
svm_pred_class = svm_result_test[[1]]
tm.svm.test = svm_result_test[[2]]
```

```{r cm svm improved}
#get the confusion matrix of the prediction
confusionMatrix(factor(svm_pred_class), dat_test$emotion_idx)
```

```{r pa svm improved}
#get the accuracy of the prediction of the svm model with improved features
svm_accuracy_test = mean(dat_test$emotion_idx == svm_pred_class)
print(paste0("The accuracy for svm model is: ", svm_accuracy_test))
```

### 4.Improved feature in XGB

```{r test xgb improved}
#prediction of the xgb model with improved features
source("../lib/xgb_test.R")
load("../output/xgb_model.RData")
load("../output/feature_test.RData")

xgb_result_test = xgb_test(xgb_model, dat_test[,-ncol(dat_test)])
xgb_pred = xgb_result_test[[1]]
tm.xgb.test = xgb_result_test[[2]]
xgb_pred_class = apply(xgb_pred, 1, which.max)-1
```

```{r cm xgb improved}
#get the confusion matrix of the prediction
confusionMatrix(factor(xgb_pred_class), dat_test$emotion_idx)
```

```{r pa xgb improved}
#get the accuracy of the prediction of the xgb model with improved features
xgb_accuracy_test = mean(dat_test$emotion_idx == xgb_pred_class)
print(paste0("The accuracy for xgb model is: ", xgb_accuracy_test))
```

### 5.Improved feature with color in GBM

```{r test gbm colored}
#prediction of the gbm model with improved features and colors features
source("../lib/test_gbm.R")
load("../output/gbm_model_color.RData")
load("../output/feature_test_color.RData")

gbm_result_test_color = gbm_test(gbm_model_color, dat_test_color)
gbm_pred_color = gbm_result_test_color[[1]]
tm.gbm.test_color = gbm_result_test_color[[2]]
gbm_pred_class_color = apply(gbm_pred_color, 1, which.max)
```

```{r cm gbm colored}
#get the confusion matrix of the prediction
confusionMatrix(factor(gbm_pred_class_color), dat_test_color$emotion_idx)
```

```{r pa gbm colored}
#get the accuracy of the prediction of the gbm model with improved features and color features
gbm_accuracy_test_color = mean(dat_test_color$emotion_idx == gbm_pred_class_color)
print(paste0("The accuracy for gbm model is: ", gbm_accuracy_test_color))
```

### 6.Improved feature with color in SVM

```{r test svm colored}
#prediction of the svm model with improved features and colors features
source("../lib/test_SVM.R")
load("../output/svm_model_color.RData")
load("../output/feature_test_color.RData")

svm_result_test_color = svm_test(svm_model_color, dat_test_color)
svm_pred_class_color = svm_result_test_color[[1]]
tm.svm.test_color = svm_result_test_color[[2]]
```

```{r cm svm colored}
#get the confusion matrix of the prediction
confusionMatrix(factor(svm_pred_class_color), dat_test_color$emotion_idx)
```

```{r pa svm colored}
#get the accuracy of the prediction of the gbm model with improved features and color features
svm_accuracy_test_color = mean(dat_test_color$emotion_idx == svm_pred_class_color)
print(paste0("The accuracy for svm model is: ", svm_accuracy_test_color))
```

### 7.Improved feature with color in XGB

```{r test xgb colored}
#prediction of the xgb model with improved features and colors features
source("../lib/xgb_test.R")
load("../output/xgb_model_color.RData")
load("../output/feature_test_color.RData")

xgb_result_test_color = xgb_test(xgb_model_color, dat_test_color[,-ncol(dat_test_color)])
xgb_pred_color = xgb_result_test_color[[1]]
tm.xgb.test_color = xgb_result_test_color[[2]]
xgb_pred_class_color = apply(xgb_pred_color, 1, which.max)-1
```

```{r cm xgb colored}
#get the confusion matrix of the prediction
confusionMatrix(factor(xgb_pred_class_color), dat_test_color$emotion_idx)
```

```{r pa xgb colored}
#get the accuracy of the prediction of the gbm model with improved features and color features
xgb_accuracy_test_color = mean(dat_test_color$emotion_idx == xgb_pred_class_color)
print(paste0("The accuracy for xgb model is: ", xgb_accuracy_test_color))
```

### Summarize Accuracy

```{r my theme}
my_theme = theme_light() + 
  theme(plot.title = element_text(hjust = 0.5), 
        plot.subtitle = element_text(hjust = 0.5)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) 
```

```{r ta vector}
#create a tibble of predict accuracy of each model
if(run.baseline){
  accuracy_test = tibble(gbm_baseline = gbm_accuracy_test_baseline, 
                         gbm_improved = gbm_accuracy_test, 
                         svm_improved = svm_accuracy_test, 
                         xgb_improed = xgb_accuracy_test, 
                         gbm_colored = gbm_accuracy_test_color, 
                         svm_colored = svm_accuracy_test_color, 
                         xgb_colored = xgb_accuracy_test_color)
}else{
  accuracy_test = tibble(gbm_improved = gbm_accuracy_test, 
                         svm_improved = svm_accuracy_test, 
                         xgb_improed  = xgb_accuracy_test, 
                         gbm_colored = gbm_accuracy_test_color, 
                         svm_colored = svm_accuracy_test_color, 
                         xgb_colored = xgb_accuracy_test_color)
}
```

```{r ta graph}
g_accuracy_test = accuracy_test %>% pivot_longer(1:ncol(accuracy_test))%>% 
  ggplot(aes(x = name, fill = name)) +
  geom_bar(aes(weight = value)) + 
  labs(x = 'Classifier', y = 'Accuracy',
       title = 'The Accuracy of Different Classifier under Different Feature') + 
  my_theme
ggsave("../figs/g_accuracy_test.jpg", plot = g_accuracy_test)
g_accuracy_test
```

### Summarize Running Time

Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited, we would want a model woth relatively high accuracy and relatively low running time. 

```{r load tm}
load("../output/tm.run.all_points.RData")
load("../output/tm_feature_train_baseline.RData")
load("../output/tm_feature_test_baseline.RData")
load("../output/tm_feature_train.RData")
load("../output/tm_feature_train_color.RData")
load("../output/tm_feature_test.RData")
load("../output/tm_feature_test_color.RData")
load("../output/tm.gbm.train_baseline.RData")
load("../output/tm.gbm.train.RData")
load("../output/tm.svm.train.RData")
load("../output/tm.xgb.train.RData")
load("../output/tm.gbm.train_color.RData")
load("../output/tm.svm.train_color.RData")
load("../output/tm.xgb.train_color.RData")
```


```{r tm not with}
#get a tibble of the running time, tm.train for the training and tm.test for the test
if(run.baseline){
  tm.train = tibble(gbm_baseline = tm.gbm.train_baseline[3], 
                    gbm_improved = tm.gbm.train[3], 
                    svm_improved = tm.svm.train[3], 
                    xgb_improved = tm.xgb.train[3], 
                    gbm_color = tm.gbm.train_color[3], 
                    svm_color = tm.svm.train_color[3], 
                    xgb_color = tm.xgb.train_color[3])
  tm.test= tibble(gbm_baseline = tm.gbm.test_baseline[3], 
                  gbm_improved = tm.gbm.test[3], 
                  svm_improved = tm.svm.test[3], 
                  xgb_improved = tm.xgb.test[3], 
                  gbm_color = tm.gbm.test_color[3], 
                  svm_color = tm.svm.test_color[3], 
                  xgb_color = tm.xgb.test_color[3])
}else{
  tm.train = tibble(gbm_improved = tm.gbm.train[3],
                    svm_improved = tm.svm.train[3], 
                    xgb_improved = tm.xgb.train[3], 
                    gbm_color = tm.gbm.train_color[3], 
                    svm_color = tm.svm.train_color[3],
                    xgb_color = tm.xgb.train_color[3])
  tm.test = tibble(gbm_improved = tm.gbm.test[3], 
                   svm_improved = tm.svm.test[3], 
                   xgb_improved = tm.xgb.test[3], 
                   gbm_color = tm.gbm.test_color[3], 
                   svm_color = tm.svm.test_color[3], 
                   xgb_color = tm.xgb.test_color[3])
}


```

```{r tm with}
#get a tibble of the total running time which used the formula at the begining of the report
#for training, total running time is the sum of data manipulation, extraction of features, and trainig
#for test, the total running time is the sum of data manipulation, extraction of feature, and testing
if(run.baseline){
  tm.train.with = tibble(gbm_baseline = tm.gbm.train_baseline[3]+tm_feature_train_baseline[3],
                    gbm_improved = tm.gbm.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8,
                    svm_improved = tm.svm.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8,
                    xgb_improved = tm.xgb.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8,
                    gbm_color = tm.gbm.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8,
                    svm_color = tm.svm.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8,
                    xgb_color = tm.xgb.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8)
  tm.test.with= tibble(gbm_baseline = tm.gbm.test_baseline[3]+tm_feature_test_baseline[3], 
                  gbm_improved = tm.gbm.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                  svm_improved = tm.svm.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                  xgb_improved = tm.xgb.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                  gbm_color = tm.gbm.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2, 
                  svm_color = tm.svm.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2, 
                  xgb_color = tm.xgb.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2)
}else{
  tm.train.with = tibble(gbm_improved = tm.gbm.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8, 
                    svm_improved = tm.svm.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8, 
                    xgb_improved = tm.xgb.train[3]+tm_feature_train[3]+tm.run.all_points[3]*0.8, 
                    gbm_color = tm.gbm.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8, 
                    svm_color = tm.svm.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8, 
                    xgb_color = tm.xgb.train_color[3]+tm_feature_train_color[3]+tm.run.all_points[3]*0.8)
  tm.test.with = tibble(gbm_improved = tm.gbm.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                   svm_improved = tm.svm.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                   xgb_improved = tm.xgb.test[3]+tm_feature_test[3]+tm.run.all_points[3]*0.2, 
                   gbm_color = tm.gbm.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2, 
                   svm_color = tm.svm.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2, 
                   xgb_color = tm.xgb.test_color[3]+tm_feature_test_color[3]+tm.run.all_points[3]*0.2)
}


```

#### Training time without feature extraction

```{r train tm graph not with}
#plot the train time without feature extraction
g_time_train = tm.train %>% pivot_longer(1:ncol(tm.train)) %>% 
  ggplot(aes(x = name, fill = name)) +
  geom_bar(aes(weight = value)) + 
  scale_y_log10() + 
  labs(x = 'Classifier', y = 'Usage Time(s)',
       title = 'Model Training time') + 
  my_theme
ggsave("../figs/g_time_train.jpg", plot = g_time_train)
g_time_train
```

#### Training time with feature extraction

```{r train tm graph with}
#plot the train time with feature extraction
g_time_train_with = tm.train.with %>% pivot_longer(1:ncol(tm.train.with)) %>% 
  ggplot(aes(x = name, fill = name)) +
  geom_bar(aes(weight = value)) + 
  scale_y_log10() + 
  labs(x = 'Classifier', y = 'Usage Time(s)',
       title = 'Feature Extraction and Model Training Time') + 
  my_theme
ggsave("../figs/g_time_train_with.jpg", plot = g_time_train_with)
g_time_train_with
```

#### Test time without feature extraction

```{r test tm graph not with}
#plot the test time without feature extraction
g_time_test = tm.test %>% pivot_longer(1:ncol(tm.test)) %>% 
  ggplot(aes(x = name, fill = name)) +
  geom_bar(aes(weight = value),) + 
  labs(x = 'Classifier', y = 'Usage time(s)', 
       title = 'Test Prediction Time') + 
  scale_y_log10() + 
  my_theme
ggsave("../figs/g_time_test.jpg", plot = g_time_test)
g_time_test
```

#### Test time with feature extraction

```{r test tm graph with}
#plot the test time with feature extraction
g_time_test_with = tm.test.with %>% pivot_longer(1:ncol(tm.test.with)) %>% 
  ggplot(aes(x = name, fill = name)) +
  geom_bar(aes(weight = value),) + 
  scale_y_log10() + 
  labs(x = 'Classifier', y = 'Usage time(s)', 
       title = 'Feature Extraction and Test Prediction Time') + 
  my_theme
ggsave("../figs/g_time_test_with.jpg", plot = g_time_test_with)
g_time_test_with
```

## Reference

- Du, S., Tao, Y., & Martinez, A. M. (2014). Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15), E1454-E1462.

- Cross-Validation on Xgboost, Fall2019 Project3 Section2 Group5. 

